## Module 22 Challenge

In this challenge, SparkSQL was used to analyze a home sales dataset, focusing on key metrics such as average home prices for various conditions. The tasks included creating temporary views, caching and uncaching data, and partitioning data by "date_built" for efficient storage and retrieval. We compared the performance of queries with and without caching to understand its impact on runtime.

The analysis demonstrated efficient data handling and optimization techniques using Spark. Key operations included calculating average prices for specific home features, managing temporary views, and verifying caching effectiveness. This challenge highlights the importance of SparkSQL in processing and analyzing large datasets efficiently.

